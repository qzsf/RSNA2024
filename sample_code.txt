Overview
train.csv: Contains the severity labels for each condition and spinal level for every study.
train_label_coordinates.csv: Provides the specific images (series_id and instance_number) and coordinates (x, y) where each condition is observed within a study.
Our goal is to:
Understand and preprocess the labels from train.csv.
Associate these labels with the corresponding images from train_label_coordinates.csv.
Prepare the final dataset where each image has associated labels ready for training.
Step 1: Load the CSV Files
First, import the necessary libraries and load the CSV files into Pandas DataFrames.
python
Copy code
import pandas as pd

# Load the CSV files
train_df = pd.read_csv('train.csv')
label_coords_df = pd.read_csv('train_label_coordinates.csv')
Step 2: Preprocess train.csv to Extract Labels
Understanding train.csv:
Each row corresponds to a study_id.
Columns represent different condition-level combinations (e.g., spinal_canal_stenosis_l1_l2).
The values are the severity levels: 'Normal/Mild', 'Moderate', or 'Severe'.
Some entries may have missing labels (NaNs).
Transforming train.csv into a Long Format:
We need to reshape train.csv from wide to long format to have one row per condition-level per study.
python
Copy code
# Melt the DataFrame to long format
train_labels_long = train_df.melt(
    id_vars=['study_id'],
    var_name='condition_level',
    value_name='severity'
)
Splitting condition_level into Condition and Level:
We need to extract the condition and the spinal level from the condition_level column.
python
Copy code
# Split 'condition_level' into 'condition' and 'level'
condition_level_split = train_labels_long['condition_level'].str.rsplit('_', n=2, expand=True)

# Handle the side (left/right) if present
train_labels_long['condition'] = condition_level_split[0]
train_labels_long['side'] = condition_level_split[1]
train_labels_long['level'] = condition_level_split[2]

# For conditions without 'left' or 'right', adjust accordingly
conditions_without_side = ['spinal', 'spinal_canal', 'spinal_canal_stenosis']
train_labels_long.loc[train_labels_long['condition'].isin(conditions_without_side), 'condition'] = (
    condition_level_split[0] + '_' + condition_level_split[1]
)
train_labels_long.loc[train_labels_long['condition'].isin(conditions_without_side), 'level'] = condition_level_split[2]
train_labels_long.loc[train_labels_long['condition'].isin(conditions_without_side), 'side'] = ''

# Combine 'condition' and 'side' where applicable
train_labels_long['condition'] = train_labels_long['condition'] + '_' + train_labels_long['side']
train_labels_long['condition'] = train_labels_long['condition'].str.rstrip('_')
Clean Up the DataFrame:
python
Copy code
# Keep only necessary columns
train_labels_long = train_labels_long[['study_id', 'condition', 'level', 'severity']]

# Replace missing severities with NaN
train_labels_long['severity'].replace('nan', pd.NA, inplace=True)
Example Result:
css
Copy code
   study_id                  condition  level       severity
0   4003253     spinal_canal_stenosis   l1_l2  Normal/Mild
1   4003253     spinal_canal_stenosis   l2_l3  Normal/Mild
...
Step 3: Preprocess train_label_coordinates.csv
Understanding train_label_coordinates.csv:
Each row corresponds to a labeled finding within a specific image.
Contains study_id, series_id, instance_number, condition, level, and coordinates (x, y).
Clean and Standardize the condition and level Columns:
Ensure that the condition and level columns in both DataFrames match.
python
Copy code
# Standardize condition names to lowercase and replace spaces with underscores
label_coords_df['condition'] = label_coords_df['condition'].str.lower().str.replace(' ', '_')
label_coords_df['level'] = label_coords_df['level'].str.lower().str.replace('/', '_')
Example Result:
python
Copy code
    study_id    series_id    instance_number   condition                level       x           y
0   4003253     702807833   8                spinal_canal_stenosis    l1_l2   322.831858 227.964602
...
Step 4: Merge Labels with Image Coordinates
Our goal is to assign the severity labels from train_labels_long to the corresponding images specified in label_coords_df.
Merge the DataFrames on study_id, condition, and level:
python
Copy code
# Merge the DataFrames
merged_df = pd.merge(
    label_coords_df,
    train_labels_long,
    on=['study_id', 'condition', 'level'],
    how='left'  # Use 'left' to keep all rows from label_coords_df
)
Check for Missing Severities:
After merging, check if there are any missing severity labels.
python
Copy code
# Identify rows with missing severity after the merge
missing_severity = merged_df['severity'].isnull()
if missing_severity.any():
    print("Some labels are missing severity information.")
Step 5: Handle Missing Labels
Depending on your strategy, you can:
Option 1: Exclude Images with Missing Labels
python
Copy code
merged_df = merged_df.dropna(subset=['severity'])
Option 2: Keep Images and Handle Missing Labels During Training
You can assign a special value (e.g., -1) to missing labels and handle them in your loss function.
python
Copy code
merged_df['severity'] = merged_df['severity'].fillna(-1)
For this example, we'll proceed with excluding images with missing labels.
Step 6: Encode Severity Labels
Map the textual severity levels to numerical values.
python
Copy code
# Map severity levels to integers
severity_mapping = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}
merged_df['severity_encoded'] = merged_df['severity'].map(severity_mapping)

# If there are any remaining missing labels, set them to -1
merged_df['severity_encoded'] = merged_df['severity_encoded'].fillna(-1)
Step 7: Prepare the Image Paths
Construct the full image paths for loading.
python
Copy code
import os

def construct_image_path(row):
    study_id = row['study_id']
    series_id = row['series_id']
    instance_number = row['instance_number']
    return os.path.join('train_images', str(study_id), str(series_id), f"{instance_number}.dcm")

merged_df['image_path'] = merged_df.apply(construct_image_path, axis=1)
Check if Image Files Exist:
python
Copy code
# Verify that the image files exist
merged_df = merged_df[merged_df['image_path'].apply(os.path.exists)]
Step 8: Prepare the Final Dataset for Training
Select Relevant Columns:
python
Copy code
final_df = merged_df[['image_path', 'condition', 'level', 'severity_encoded']]
Optionally, Combine condition and level into a Single Label:
python
Copy code
# Create a combined label
final_df['label'] = final_df['condition'] + '_' + final_df['level']
Example of the Final DataFrame:
python
Copy code
    image_path                                         label                                severity_encoded
0   train_images/4003253/702807833/8.dcm    spinal_canal_stenosis_l1_l2       0
1   train_images/4003253/702807833/8.dcm    spinal_canal_stenosis_l2_l3       0
...
Step 9: Convert Labels to Suitable Format for Training
Depending on your modeling approach, you might:
Option A: Treat as Multi-Class Classification
If you decide to classify each condition-level separately, you can proceed by:
Creating a Multi-Class Label Encoder:
python
Copy code
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Encode the 'severity_encoded' as categories (0, 1, 2)
y = final_df['severity_encoded'].values.astype(int)
y = to_categorical(y, num_classes=3)
Option B: Treat as Multi-Label Classification
If you want to consider all labels for an image at once, you can pivot the DataFrame.
python
Copy code
# Create a pivot table with image paths as index and labels as columns
pivot_df = final_df.pivot_table(
    index='image_path',
    columns='label',
    values='severity_encoded',
    aggfunc='first'  # Since each image-label pair is unique
)

# Fill missing labels with -1 or any other sentinel value
pivot_df = pivot_df.fillna(-1)
Converting the Pivot Table to Arrays:
python
Copy code
# Convert pivot_df to a NumPy array
labels_array = pivot_df.values  # Shape: (num_images, num_labels)

# Get the list of image paths
image_paths = pivot_df.index.tolist()
Handle Missing Labels in the Array:
Option 1: Mask Missing Labels During Training
You can handle missing labels by masking them during loss calculation (explained later).
Option 2: Exclude Images with Missing Labels
python
Copy code
# Exclude images with any missing labels
valid_indices = ~np.any(labels_array == -1, axis=1)
labels_array = labels_array[valid_indices]
image_paths = [image_paths[i] for i in range(len(image_paths)) if valid_indices[i]]
Step 10: Load Images and Prepare for Training
Load Images Using pydicom:
python
Copy code
import numpy as np
import pydicom
from tensorflow.keras.preprocessing.image import img_to_array

def load_and_preprocess_image(image_path):
    dicom = pydicom.dcmread(image_path)
    image = dicom.pixel_array.astype(np.float32)
    # Normalize the image
    image = (image - np.min(image)) / (np.max(image) - np.min(image))
    # Convert to RGB by stacking the grayscale image
    image = np.stack((image,)*3, axis=-1)
    # Resize the image to match the input size of the model (e.g., 224x224)
    image = tf.image.resize(image, [224, 224]).numpy()
    return image

# Load all images
images = [load_and_preprocess_image(path) for path in image_paths]
images_array = np.array(images)
Step 11: Split Data into Training and Validation Sets
python
Copy code
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    images_array,
    labels_array,
    test_size=0.2,
    random_state=42
)
Step 12: Build and Compile Your Model
For Multi-Label Classification:
python
Copy code
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16

num_labels = labels_array.shape[1]  # Number of labels

# Build the model
input_layer = Input(shape=(224, 224, 3))
base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_layer)
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
output_layer = Dense(num_labels, activation='sigmoid')(x)  # Use 'sigmoid' for multi-label

model = Model(inputs=input_layer, outputs=output_layer)
Compile the Model with a Custom Loss Function to Handle Missing Labels:
python
Copy code
def masked_loss(y_true, y_pred):
    # Mask for labels not equal to -1
    mask = tf.not_equal(y_true, -1)
    # Apply binary crossentropy only on valid labels
    loss = tf.keras.losses.binary_crossentropy(y_true * tf.cast(mask, tf.float32), y_pred * tf.cast(mask, tf.float32))
    # Compute mean loss
    return tf.reduce_mean(loss)

model.compile(optimizer='adam', loss=masked_loss, metrics=['accuracy'])
Step 13: Train the Model
python
Copy code
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32
)
Alternative Approach: Handling Each Condition-Level Separately
If you prefer to train separate models or outputs for each condition-level, you can adjust your model accordingly.
Creating Multiple Outputs:
python
Copy code
outputs = []
for i in range(num_labels):
    outputs.append(Dense(1, activation='sigmoid', name=f'output_{i}')(x))

model = Model(inputs=input_layer, outputs=outputs)
Compile the Model with Multiple Losses:
python
Copy code
losses = {f'output_{i}': masked_loss for i in range(num_labels)}
model.compile(optimizer='adam', loss=losses)
Prepare Labels as a Dictionary:
python
Copy code
# Convert labels_array to a dictionary
y_train_dict = {f'output_{i}': y_train[:, i] for i in range(num_labels)}
y_val_dict = {f'output_{i}': y_val[:, i] for i in range(num_labels)}
Train the Model:
python
Copy code
history = model.fit(
    X_train, y_train_dict,
    validation_data=(X_val, y_val_dict),
    epochs=10,
    batch_size=32
)
Summary
Use train.csv to get the severity labels for each condition and spinal level per study.
Use train_label_coordinates.csv to find out which images (specified by series_id and instance_number) correspond to each condition and level.
Merge the two datasets on study_id, condition, and level to associate severity labels with specific images.
Handle missing labels appropriately, either by excluding them or by using masking during training.
Encode the severity labels into numerical values suitable for training your model.
Load the images using the constructed paths and preprocess them for input into your model.
Prepare the data for training, splitting into training and validation sets.
Build and compile your model, ensuring that it can handle multi-label classification and missing labels.
Train the model and evaluate its performance.




Check if the dataset is imbalanced. To count the number of samples for each severity level in your dataset using Python, you can use the value_counts() method provided by Pandas. Here's how you can do it:
python
Copy code
import pandas as pd

# Assuming 'final_df' is your DataFrame and 'severity' is the column with severity labels
severity_counts = final_df['severity'].value_counts()

# Display the counts
print(severity_counts)
Explanation:
final_df['severity']: Accesses the severity column in your DataFrame.
.value_counts(): Counts the number of occurrences of each unique value in the severity column.
print(severity_counts): Outputs the count of each severity level.
Example Output:
yaml
Copy code
Normal/Mild    2000
Moderate        800
Severe          200
Name: severity, dtype: int64
This output tells you how many samples there are for each severity category in your dataset.
Additional Tips:
Sorting the Counts:
If you want to sort the counts in ascending order, you can pass the sort parameter:
python
Copy code
severity_counts = final_df['severity'].value_counts(sort=True)
Accessing Counts Individually:
To access the count of a specific severity level:
python
Copy code
normal_mild_count = severity_counts.get('Normal/Mild', 0)
moderate_count = severity_counts.get('Moderate', 0)
severe_count = severity_counts.get('Severe', 0)

print(f"Normal/Mild: {normal_mild_count}")
print(f"Moderate: {moderate_count}")
print(f"Severe: {severe_count}")
Visualizing the Distribution:
You can also visualize the distribution using a bar chart:
python
Copy code
import matplotlib.pyplot as plt

severity_counts.plot(kind='bar')
plt.title('Number of Samples per Severity Level')
plt.xlabel('Severity')
plt.ylabel('Count')
plt.show()
Using GroupBy for More Complex Counts:
If your dataset has multiple conditions or levels and you want to see counts per severity per condition or level, you can use groupby:
python
Copy code
# Count samples per severity and condition
severity_condition_counts = final_df.groupby(['condition', 'severity']).size().reset_index(name='counts')
print(severity_condition_counts)
Example Output:
css
Copy code
                 condition      severity  counts
0  left_neural_foraminal_narrowing  Moderate     150
1  left_neural_foraminal_narrowing  Normal/Mild  500
2  left_neural_foraminal_narrowing  Severe        50
3  spinal_canal_stenosis            Moderate     200
4  spinal_canal_stenosis            Normal/Mild 1000
5  spinal_canal_stenosis            Severe       100
This provides a breakdown of counts per severity level within each condition.



Here's a step-by-step guide:
Import Necessary Libraries
Load and Preprocess Images
Prepare Labels
Split Data into Training and Validation Sets
Build the Model
Train the Model
Evaluate the Model
Step 1: Import Necessary Libraries
python
Copy code
import os
import numpy as np
import pandas as pd
import pydicom
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.layers import Input, Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
Step 2: Load and Preprocess Images
2.1. Load the DataFrame
Assuming you have your final dataset in a DataFrame called final_df:
python
Copy code
# Load your DataFrame
final_df = pd.read_csv('your_final_dataset.csv')  # Replace with your actual file or DataFrame
2.2. Define a Function to Load and Preprocess Images
python
Copy code
def load_and_preprocess_dicom(row):
    dicom_path = os.path.join(
        'train_images',
        str(row['study_id']),
        str(row['series_id']),
        f"{row['instance_number']}.dcm"
    )
    # Read the DICOM file
    dicom = pydicom.dcmread(dicom_path)
    image = dicom.pixel_array.astype(np.float32)
    
    # Normalize the image (0 to 1)
    image = (image - np.min(image)) / (np.max(image) - np.min(image))
    
    # Resize the image to 224x224 (VGG16 input size)
    image = tf.image.resize(image, [224, 224]).numpy()
    
    # Convert grayscale image to RGB by stacking the grayscale image three times
    image = np.stack((image,)*3, axis=-1)
    
    return image
2.3. Load All Images
python
Copy code
# Initialize lists to store images and labels
images = []
labels = []

# Iterate over the DataFrame and load images
for idx, row in final_df.iterrows():
    try:
        image = load_and_preprocess_dicom(row)
        images.append(image)
        labels.append(row['severity'])
    except Exception as e:
        print(f"Error loading image for row {idx}: {e}")
        continue

# Convert lists to NumPy arrays
images = np.array(images)
Step 3: Prepare Labels
3.1. Encode Labels
python
Copy code
# Encode severity labels to integers
severity_mapping = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}
labels_encoded = [severity_mapping[label] for label in labels]
labels_encoded = np.array(labels_encoded)
3.2. Convert Labels to Categorical (One-Hot Encoding)
python
Copy code
labels_categorical = to_categorical(labels_encoded, num_classes=3)
Step 4: Split Data into Training and Validation Sets
python
Copy code
X_train, X_val, y_train, y_val = train_test_split(
    images,
    labels_categorical,
    test_size=0.2,
    random_state=42,
    stratify=labels_encoded  # Stratify to maintain label distribution
)
Step 5: Build the Model
5.1. Load Pre-trained VGG16 Model
python
Copy code
# Load VGG16 model without the top layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
5.2. Freeze Base Model Layers
python
Copy code
for layer in base_model.layers:
    layer.trainable = False
5.3. Add Custom Layers
python
Copy code
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(3, activation='softmax')(x)  # 3 classes for severity levels
5.4. Create the Model
python
Copy code
model = Model(inputs=base_model.input, outputs=predictions)
5.5. Compile the Model
python
Copy code
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
Step 6: Train the Model
python
Copy code
epochs = 10
batch_size = 32

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True
)
Step 7: Evaluate the Model
python
Copy code
# Evaluate on the validation set
val_loss, val_accuracy = model.evaluate(X_val, y_val)
print(f"Validation Accuracy: {val_accuracy}")
Additional Considerations
Handling Class Imbalance
If your dataset has class imbalance, you can compute class weights and pass them to model.fit().
python
Copy code
from sklearn.utils import class_weight

# Compute class weights
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(labels_encoded),
    y=labels_encoded
)

class_weights_dict = dict(enumerate(class_weights))

# Pass class_weights to model.fit()
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True,
    class_weight=class_weights_dict
)
Fine-Tuning the Base Model
After initial training, you can unfreeze some layers of the base model to fine-tune it.
Unfreeze Layers
python
Copy code
# Unfreeze the last few layers
for layer in base_model.layers[-4:]:
    layer.trainable = True
Recompile the Model with a Lower Learning Rate
python
Copy code
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
Retrain the Model
python
Copy code
history_fine = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=5,
    batch_size=batch_size,
    shuffle=True,
    class_weight=class_weights_dict
)
Data Augmentation
Since we're not using ImageDataGenerator, you can perform data augmentation manually.
Define Augmentation Function
python
Copy code
import tensorflow as tf

def augment_image(image):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=0.1)
    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)
    image = tf.image.rot90(image, k=np.random.randint(4))  # Rotate 0, 90, 180, or 270 degrees
    return image
Apply Augmentation to Training Images
python
Copy code
augmented_images = []
augmented_labels = []

for i in range(len(X_train)):
    augmented_images.append(X_train[i])
    augmented_labels.append(y_train[i])
    
    # Augment the image
    augmented_img = augment_image(X_train[i])
    augmented_images.append(augmented_img.numpy())
    augmented_labels.append(y_train[i])

# Convert lists to arrays
X_train_augmented = np.array(augmented_images)
y_train_augmented = np.array(augmented_labels)
Retrain the Model with Augmented Data
python
Copy code
history = model.fit(
    X_train_augmented, y_train_augmented,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True,
    class_weight=class_weights_dict
)
Potential Issues and Solutions
Memory Constraints
Loading all images into memory may cause memory issues, especially with large datasets.
Solution:
Use a custom data generator (tf.keras.utils.Sequence) that loads images in batches during training.
Example:
python
Copy code
from tensorflow.keras.utils import Sequence

class DataGenerator(Sequence):
    def __init__(self, dataframe, batch_size=32, shuffle=True):
        self.dataframe = dataframe
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(self.dataframe))
        self.on_epoch_end()
    
    def __len__(self):
        return int(np.ceil(len(self.dataframe) / self.batch_size))
    
    def __getitem__(self, index):
        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        batch_df = self.dataframe.iloc[batch_indexes]
        
        images = []
        labels = []
        
        for idx, row in batch_df.iterrows():
            image = load_and_preprocess_dicom(row)
            images.append(image)
            label = severity_mapping[row['severity']]
            labels.append(label)
        
        images = np.array(images)
        labels = to_categorical(labels, num_classes=3)
        return images, labels
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)
Use the Data Generator
python
Copy code
# Create training and validation DataFrames
train_df, val_df = train_test_split(final_df, test_size=0.2, random_state=42, stratify=final_df['severity'])

# Create Data Generators
train_generator = DataGenerator(train_df, batch_size=batch_size, shuffle=True)
val_generator = DataGenerator(val_df, batch_size=batch_size, shuffle=False)

# Train the model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=epochs,
    class_weight=class_weights_dict
)
Conclusion
By following this approach, you can fine-tune a VGG16 model using your DICOM images without the need to convert them to PNG or use ImageDataGenerator. This method involves:
Loading DICOM images directly using pydicom.
Preprocessing the images to match the input requirements of VGG16.
Preparing the labels for training.
Training the model using standard TensorFlow/Keras functions.
Let Me Know If You Need Further Assistance
Feel free to ask if you have any questions or need help with specific parts of the code. I'm here to help you through the process!
